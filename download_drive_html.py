
import os
import json
import sys
import io
import random
from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload

# ------------------------
# æœåŠ¡è´¦å·é…ç½®
# ------------------------
# ä»ç¯å¢ƒå˜é‡ä¸­è·å–æœåŠ¡è´¦å·ä¿¡æ¯
service_account_info = os.environ.get("GDRIVE_SERVICE_ACCOUNT")
if not service_account_info:
    print("âŒ æœªæ‰¾åˆ° GDRIVE_SERVICE_ACCOUNT ç¯å¢ƒå˜é‡ã€‚")
    sys.exit(1)

# ä» JSON å­—ç¬¦ä¸²åŠ è½½æœåŠ¡è´¦å·å‡­è¯
try:
    service_account_info = json.loads(service_account_info)
except json.JSONDecodeError:
    print("âŒ è§£æ GDRIVE_SERVICE_ACCOUNT å¤±è´¥ã€‚è¯·ç¡®ä¿å®ƒæ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„ JSON å­—ç¬¦ä¸²ã€‚")
    sys.exit(1)

# å®šä¹‰ Google Drive çš„åªè¯»æƒé™èŒƒå›´
SCOPES = ['https://www.googleapis.com/auth/drive.readonly']
# æ ¹æ®æœåŠ¡è´¦å·ä¿¡æ¯åˆ›å»ºå‡­è¯
creds = service_account.Credentials.from_service_account_info(service_account_info, scopes=SCOPES)
# æ„å»º Google Drive API æœåŠ¡
service = build('drive', 'v3', credentials=creds)

# ------------------------
# æ”¯æŒå¤šæ–‡ä»¶å¤¹ ID
# ------------------------
# ä»ç¯å¢ƒå˜é‡è·å–æ–‡ä»¶å¤¹ ID
folder_ids_str = os.environ.get("GDRIVE_FOLDER_ID")
if not folder_ids_str:
    print("âŒ æœªæ‰¾åˆ° GDRIVE_FOLDER_ID ç¯å¢ƒå˜é‡ã€‚")
    sys.exit(1)

# å°†æ–‡ä»¶å¤¹ ID å­—ç¬¦ä¸²æ‹†åˆ†ä¸ºåˆ—è¡¨ï¼Œå¹¶å»é™¤å¤šä½™çš„ç©ºæ ¼
FOLDER_IDS = [fid.strip() for fid in folder_ids_str.split(",") if fid.strip()]

# ------------------------
# ä» TXT æ–‡ä»¶è¯»å–å…³é”®è¯
# ------------------------
keywords = []
keywords_file = "keywords.txt"
if os.path.exists(keywords_file):
    with open(keywords_file, "r", encoding="utf-8") as f:
        # è¯»å–æ–‡ä»¶ä¸­çš„æ¯ä¸€è¡Œï¼Œå»é™¤é¦–å°¾ç©ºæ ¼ï¼Œå¹¶æ·»åŠ åˆ°å…³é”®è¯åˆ—è¡¨
        keywords = [line.strip() for line in f if line.strip()]

if not keywords:
    print("âš ï¸ keywords.txt ä¸­æ²¡æœ‰æ‰¾åˆ°å…³é”®è¯ï¼Œå°†ä½¿ç”¨åŸå§‹æ–‡ä»¶åã€‚")

# ------------------------
# è®°å½•å·²å¤„ç†çš„æ–‡ä»¶ ID
# ------------------------
processed_file_path = "processed_files.json"
try:
    if os.path.exists(processed_file_path):
        with open(processed_file_path, "r") as f:
            processed_data = json.load(f)
    else:
        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„æ•°æ®ç»“æ„
        processed_data = {"fileIds": []}
except (json.JSONDecodeError, IOError) as e:
    print(f"è¯»å– {processed_file_path} æ—¶å‡ºé”™: {e}ã€‚å°†ä»ä¸€ä¸ªç©ºçš„å·²å¤„ç†æ–‡ä»¶åˆ—è¡¨å¼€å§‹ã€‚")
    processed_data = {"fileIds": []}


# ------------------------
# è·å–æ–‡ä»¶åˆ—è¡¨çš„å‡½æ•°
# ------------------------
def list_files(folder_id):
    """åˆ—å‡ºæŒ‡å®š Google Drive æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶ï¼Œæ”¯æŒåˆ†é¡µã€‚"""
    all_the_files = []
    page_token = None
    page_count = 0
    # æŸ¥è¯¢ç‰¹å®š MIME ç±»å‹çš„æ–‡ä»¶
    query = f"'{folder_id}' in parents and (" \
            "mimeType='text/html' or " \
            "mimeType='text/plain' or " \
            "mimeType='application/vnd.google-apps.document')"
    try:
        while True:
            page_count += 1
            print(f"  - æ­£åœ¨è·å–ç¬¬ {page_count} é¡µæ–‡ä»¶...")
            results = service.files().list(
                q=query,
                pageSize=1000,
                # é‡è¦ï¼šfields å¿…é¡»åŒ…å« nextPageToken æ‰èƒ½åœ¨å“åº”ä¸­è·å–å®ƒ
                fields="nextPageToken, files(id, name, mimeType)",
                pageToken=page_token
            ).execute()
            
            items = results.get('files', [])
            all_the_files.extend(items)
            
            page_token = results.get('nextPageToken', None)
            if page_token is None:
                break # æ²¡æœ‰æ›´å¤šé¡µé¢äº†ï¼Œé€€å‡ºå¾ªç¯
        
        print(f"  - åœ¨æ–‡ä»¶å¤¹ {folder_id} ä¸­æ€»å…±æ‰¾åˆ° {len(all_the_files)} ä¸ªæ–‡ä»¶ã€‚")
        return all_the_files
        
    except Exception as e:
        print(f"åˆ—å‡ºæ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return []

# ------------------------
# ä¸‹è½½å’Œç”Ÿæˆ HTML
# ------------------------
def download_html_file(file_id, file_name):
    """ä¸‹è½½ä¸€ä¸ª HTML æ–‡ä»¶ã€‚"""
    request = service.files().get_media(fileId=file_id)
    fh = io.FileIO(file_name, 'wb')
    downloader = MediaIoBaseDownload(fh, request)
    done = False
    while not done:
        _, done = downloader.next_chunk()
    print(f"âœ… å·²ä¸‹è½½ {file_name}")

def download_txt_file(file_id, file_name, original_name):
    """ä¸‹è½½ä¸€ä¸ªæ–‡æœ¬æ–‡ä»¶å¹¶å°†å…¶è½¬æ¢ä¸º HTMLã€‚"""
    request = service.files().get_media(fileId=file_id)
    fh = io.BytesIO()
    downloader = MediaIoBaseDownload(fh, request)
    done = False
    while not done:
        _, done = downloader.next_chunk()
    text_content = fh.getvalue().decode('utf-8')
    # ä»æ–‡æœ¬å†…å®¹åˆ›å»º HTML
    html_content = f"<!DOCTYPE html><html><head><meta charset='utf-8'><title>{original_name}</title></head><body><pre>{text_content}</pre></body></html>"
    with open(file_name, 'w', encoding='utf-8') as f:
        f.write(html_content)
    print(f"âœ… TXT å·²è½¬æ¢ä¸º HTML: {file_name}")

def export_google_doc(file_id, file_name):
    """å°† Google æ–‡æ¡£å¯¼å‡ºä¸º HTMLã€‚"""
    request = service.files().export_media(fileId=file_id, mimeType='text/html')
    fh = io.FileIO(file_name, 'wb')
    downloader = MediaIoBaseDownload(fh, request)
    done = False
    while not done:
        _, done = downloader.next_chunk()
    print(f"âœ… Google æ–‡æ¡£å·²å¯¼å‡ºä¸º HTML: {file_name}")

# ------------------------
# ä¸»ç¨‹åº
# ------------------------
all_files = []
for folder_id in FOLDER_IDS:
    print(f"ğŸ“‚ æ­£åœ¨ä»æ–‡ä»¶å¤¹è·å–æ–‡ä»¶: {folder_id}")
    files = list_files(folder_id)
    # è¿‡æ»¤æ‰å·²ç»å¤„ç†è¿‡çš„æ–‡ä»¶
    new_files = [f for f in files if f['id'] not in processed_data["fileIds"]]
    all_files.extend(new_files)

if not all_files:
    print("âœ… æ²¡æœ‰æ–°çš„æ–‡ä»¶éœ€è¦å¤„ç†ã€‚")
    sys.exit(0)

print(f"å‘ç° {len(all_files)} ä¸ªæ–°æ–‡ä»¶éœ€è¦å¤„ç†ã€‚")

# éšæœºé€‰æ‹©æœ€å¤š 30 ä¸ªæ–‡ä»¶è¿›è¡Œå¤„ç†
num_to_process = min(len(all_files), 30)
selected_files = random.sample(all_files, num_to_process)

print(f"æœ¬æ¬¡è¿è¡Œå°†å¤„ç† {len(selected_files)} ä¸ªæ–‡ä»¶ã€‚")

# åˆ›å»ºä¸€ä¸ªå…³é”®è¯çš„å‰¯æœ¬ï¼Œç”¨äºæ¶ˆè€—
available_keywords = list(keywords)
keywords_ran_out = False

for f in selected_files:
    # å¦‚æœè¿˜æœ‰å¯ç”¨çš„å…³é”®è¯ï¼Œåˆ™ä½¿ç”¨ç¬¬ä¸€ä¸ªå¹¶å°†å…¶ä»åˆ—è¡¨ä¸­ç§»é™¤
    if available_keywords:
        keyword = available_keywords.pop(0)
        safe_name = keyword + ".html"
    else:
        # **å…³é”®æ”¹åŠ¨**: å…³é”®è¯ç”¨å®Œåï¼Œä½¿ç”¨åŸå§‹æ–‡ä»¶å + éšæœºåç¼€
        if not keywords_ran_out:
            print("âš ï¸ å…³é”®è¯å·²ç”¨å®Œï¼Œå°†ä½¿ç”¨åŸå§‹æ–‡ä»¶ååŠ éšæœºåç¼€ã€‚")
            keywords_ran_out = True
        
        # 1. æ¸…ç†åŸå§‹æ–‡ä»¶åï¼Œç§»é™¤å¯èƒ½å­˜åœ¨çš„æ‰©å±•å
        base_name = os.path.splitext(f['name'])[0]
        sanitized_name = base_name.replace(" ", "-").replace("/", "-")
        # 2. ç”Ÿæˆä¸€ä¸ª4ä½çš„éšæœºæ•°å­—åç¼€
        random_suffix = str(random.randint(1000, 9999))
        # 3. ç»„åˆæˆæœ€ç»ˆæ–‡ä»¶å
        safe_name = f"{sanitized_name}-{random_suffix}.html"

    print(f"æ­£åœ¨å¤„ç† '{f['name']}' -> '{safe_name}'")

    # æ ¹æ®æ–‡ä»¶çš„ MIME ç±»å‹ä¸‹è½½æˆ–å¯¼å‡ºæ–‡ä»¶
    if f['mimeType'] == 'text/html':
        download_html_file(f['id'], safe_name)
    elif f['mimeType'] == 'text/plain':
        download_txt_file(f['id'], safe_name, f['name'])
    else: # 'application/vnd.google-apps.document'
        export_google_doc(f['id'], safe_name)

    # å°†æ–‡ä»¶ ID æ·»åŠ åˆ°å·²å¤„ç†æ–‡ä»¶åˆ—è¡¨
    processed_data["fileIds"].append(f['id'])

# ä¿å­˜æ›´æ–°åçš„å·²å¤„ç†æ–‡ä»¶ ID åˆ—è¡¨
with open(processed_file_path, "w") as f:
    json.dump(processed_data, f, indent=4)
print(f"ğŸ’¾ å·²å°† {len(selected_files)} ä¸ªæ–°æ–‡ä»¶ ID ä¿å­˜åˆ° {processed_file_path}")

# å°†å‰©ä½™çš„å…³é”®è¯å†™å› keywords.txt æ–‡ä»¶
with open(keywords_file, "w", encoding="utf-8") as f:
    for keyword in available_keywords:
        f.write(keyword + "\n")
print(f"âœ… å·²ç”¨å‰©ä½™çš„å…³é”®è¯æ›´æ–° {keywords_file}")


# ------------------------
# ç”Ÿæˆç´¯ç§¯çš„ç«™ç‚¹åœ°å›¾
# ------------------------
# è·å–ç›®å½•ä¸­æ‰€æœ‰ç°æœ‰çš„ HTML æ–‡ä»¶
existing_html_files = [f for f in os.listdir(".") if f.endswith(".html") and f != "index.html"]
index_content = "<!DOCTYPE html><html><head><meta charset='utf-8'><title>Reading Glasses</title></head><body>\n"
index_content += "<h1>Reading Glasses</h1>\n<ul>\n"
# ä¸ºæ¯ä¸ª HTML æ–‡ä»¶åœ¨ç´¢å¼•ä¸­æ·»åŠ ä¸€ä¸ªé“¾æ¥
for fname in sorted(existing_html_files):
    index_content += f'<li><a href="{fname}">{fname}</a></li>\n'
index_content += "</ul>\n</body></html>"

with open("index.html", "w", encoding="utf-8") as f:
    f.write(index_content)
print("âœ… å·²ç”Ÿæˆ index.html (å®Œæ•´ç«™ç‚¹åœ°å›¾)")

# ------------------------
# åœ¨æ¯ä¸ªé¡µé¢åº•éƒ¨æ·»åŠ éšæœºå†…éƒ¨é“¾æ¥
# ------------------------
all_html_files = [f for f in os.listdir(".") if f.endswith(".html") and f != "index.html"]

for fname in all_html_files:
    try:
        # è¯»å– HTML æ–‡ä»¶çš„å†…å®¹
        with open(fname, "r", encoding="utf-8", errors="replace") as f:
            content = f.read()

        # ä»æ½œåœ¨é“¾æ¥åˆ—è¡¨ä¸­æ’é™¤å½“å‰æ–‡ä»¶
        other_files = [x for x in all_html_files if x != fname]
        # ç¡®å®šè¦æ·»åŠ çš„éšæœºé“¾æ¥æ•°é‡ï¼ˆ4 åˆ° 6 ä¸ªä¹‹é—´ï¼‰
        num_links = min(len(other_files), random.randint(4, 6))

        if num_links > 0:
            # éšæœºé€‰æ‹©è¦é“¾æ¥çš„æ–‡ä»¶
            random_links = random.sample(other_files, num_links)
            # ä¸ºé¡µè„šé“¾æ¥åˆ›å»º HTML
            links_html = "<footer><ul>\n" + "\n".join([f'<li><a href="{x}">{x}</a></li>' for x in random_links]) + "\n</ul></footer>"
            # å°†é“¾æ¥é™„åŠ åˆ°æ–‡ä»¶å†…å®¹
            content += links_html

        # å°†æ›´æ–°åçš„å†…å®¹å†™å›æ–‡ä»¶
        with open(fname, "w", encoding="utf-8") as f:
            f.write(content)
    except Exception as e:
        print(f"æ— æ³•ä¸º {fname} å¤„ç†å†…éƒ¨é“¾æ¥: {e}")

print("âœ… å·²ä¸ºæ‰€æœ‰é¡µé¢æ›´æ–°åº•éƒ¨éšæœºå†…éƒ¨é“¾æ¥ (æ¯ä¸ª 4-6 ä¸ªï¼Œå®Œå…¨åˆ·æ–°)")
